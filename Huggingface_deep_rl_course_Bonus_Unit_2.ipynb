{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPIKsFeGw/cMcmLwvOLSIhC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bhmMcmmwBr19"},"outputs":[],"source":["!pip install stable-baselines3"]},{"cell_type":"code","source":["# Optional: install SB3 contrib to have access to additional algorithms\n","!pip install sb3-contrib"],"metadata":{"id":"nRH0rcFGE0ZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optuna will be used in the last part when doing hyperparameter tuning\n","!pip install optuna"],"metadata":{"id":"OmAlVvjpFJ-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np"],"metadata":{"id":"XSV4zEQ7FM49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"],"metadata":{"id":"CNE5ZuegFPGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Algorithms from the contrib repo\n","# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n","from sb3_contrib import QRDQN, TQC"],"metadata":{"id":"5liwozmcFxiD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy"],"metadata":{"id":"LS-vA1FjF9n7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env_id = \"Pendulum-v1\"\n","# Env used only for evaluation\n","eval_envs = make_vec_env(env_id, n_envs=10)\n","# 4000 training timesteps\n","budget_pendulum = 4000"],"metadata":{"id":"_TMwMHPwGG_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"],"metadata":{"id":"ZiAZNoFYJsL7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n","\n","print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"5vpEsev2J8Qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define and train a A2C model\n","a2c_model = A2C(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(budget_pendulum)"],"metadata":{"id":"dI2a7UjFKGq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the train A2C model\n","mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)\n","\n","print(f\"A2C Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"2y0XuufGKd-U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train longer\n","new_budget = 10 * budget_pendulum\n","\n","ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(new_budget)"],"metadata":{"id":"SKL5IGESKsPL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n","\n","print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"tETQr-FRNqEN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tuned_params = {\n","        \"gamma\": 0.9,\n","        \"use_sde\": True,\n","        \"sde_sample_freq\": 4,\n","        \"learning_rate\": 1e-3,\n","        }\n","\n","# budget = 10 * budget_pendulum\n","ppo_tuned_model = PPO(\"MlpPolicy\", env_id, seed=1, verbose=1, **tuned_params).learn(50_000, log_interval=5)"],"metadata":{"id":"caGB7Ul8N_lF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)\n","\n","print(f\"Tuned PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"Nq-cWaQcQZlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["budget = 20_000"],"metadata":{"id":"E_1dx4A_QiCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_envs_cartpole = make_vec_env(\"CartPole-v1\", n_envs=10)"],"metadata":{"id":"lcy27oVrTf6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"],"metadata":{"id":"5hDhJ3mRT8jV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"cRQcZ9SXUD-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"osHcmRvdUbWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["policy_kwargs = dict(\n","            net_arch=[\n","                dict(vf=[64, 64], pi=[64, 64]), # network architectures for actor/critic\n","                ],\n","            activation_fn=nn.Tanh,\n","            )\n","\n","hyperparams = dict(\n","          n_steps=5,        # number of steps to collect data before updating policy\n","          learning_rate=7e-4,\n","          gamma=0.99,        # discount factor\n","          max_grad_norm=0.5,    # The maximum value for the gradient clipping\n","          ent_coef=0.0,       # Entropy coefficient for the loss calculation\n","          )\n","\n","model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"],"metadata":{"id":"K4nWwNpsVQU0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n","\n","print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"],"metadata":{"id":"eCFCeQ5zY4Ll"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gamma = trial.suggest_float(\"gamma\", 0.9, 0.99999, log=True)\n","max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n","# from 2**3 = 8 to 2**10 = 1024\n","n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n","learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n","ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n","# net_arch tiny: {\"pi\": [64], \"vf\": [64]}\n","# net_arch default: {\"pi\": [64, 64], \"vf\": [64, 64]}\n","# activation_fn = nn.Tanh / nn.ReLU"],"metadata":{"id":"hmBKdDtEZ8Q9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","from optuna.pruners import MedianPruner\n","from optuna.samplers import TPESampler\n","from optuna.visualization import plot_optimization_history, plot_param_importances"],"metadata":{"id":"61FzeNkodV_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_TRIALS = 100       # Maximum number of trials\n","N_JOBS = 1         # Number of jobs to run in parallel\n","N_STARTUP_TRIALS = 5    # Stop random sampling after N_STARTUP_TRIALS\n","N_EVALUATIONS = 2     # Number of evaluations during the training\n","N_TIMESTEPS = int(2e4)   # Training budget\n","EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n","N_EVAL_ENVS = 5\n","N_EVAL_EPISODES = 10\n","TIMEOUT = int(60 * 15)  # 15 minutes\n","\n","ENV_ID = \"CartPole-v1\"\n","\n","DEFAULT_HYPERPARAMS = {\n","            \"policy\": \"MlpPolicy\",\n","            \"env\": ENV_ID,\n","            }"],"metadata":{"id":"vdL4eIYgeW9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Any, Dict\n","import torch\n","import torch.nn as nn\n","\n","def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n","  \"\"\"\n","  Sampler for A2C hyperparameters.\n","\n","  :param trial: Optuna trial object\n","  :return: The sampled hyperparameters for the given trial.\n","  \"\"\"\n","  # Discount factor between 0.9 and 0.9999\n","  gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n","  max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n","  # 8, 16, 32, ... 1024\n","  n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n","\n","  ### YOUR CODE HERE\n","  # TODO:\n","  # - define the learning rate search space [1e-5, 1] (log) -> `suggest_float`\n","  # - define the network architecture search space [\"tiny\", \"small\"] -> `suggest_categorical`\n","  # - define the activation function search space [\"tanh\", \"relu\"]\n","  learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n","  net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n","  activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n","\n","  ### END OF YOUR CODE\n","\n","  # Display true values\n","  trial.set_user_attr(\"gamma_\", gamma)\n","  trial.set_user_attr(\"n_steps\", n_steps)\n","\n","  net_arch = [\n","        {\"pi\": [64], \"vf\": [64]}\n","        if net_arch == \"tiny\"\n","        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n","        if net_arch == \"small\"\n","        else trial.suggest_categorical(\"net_arch\", [{\"pi\": [64, 64], \"vf\": [64, 64]}], )\n","        ]\n","\n","  activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n","\n","  return {\n","      \"n_steps\": n_steps,\n","      \"gamma\": gamma,\n","      \"learning_rate\": learning_rate,\n","      \"max_grad_norm\": max_grad_norm,\n","      \"policy_kwargs\": {\n","                \"net_arch\": net_arch,\n","                \"activation_fn\": activation_fn,\n","               },\n","      }"],"metadata":{"id":"oyvZy1fUevwm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","\n","class TrialEvalCallback(EvalCallback):\n","  \"\"\"\n","  Callback used for evaluating and reporting a trial.\n","\n","  :param eval_env:      Evaluation environement\n","  :param trial:        Optuna trial object\n","  :param n_eval_episodes:   Number of evaluation episodes\n","  :param eval_freq:      Evaluate the agent every ``eval_freq`` call of the callback.\n","  :param deterministic:    Whether the evaluation should use a stochastic or deterministic policy.\n","  :param verbose:\n","  \"\"\"\n","\n","  def __init__(\n","        self,\n","        eval_env: gym.Env,\n","        trial: optuna.Trial,\n","        n_eval_episodes: int = 5,\n","        eval_freq: int = 10000,\n","        deterministic: bool = True,\n","        verbose: int = 0,\n","        ):\n","\n","    super().__init__(\n","            eval_env=eval_env,\n","            n_eval_episodes=n_eval_episodes,\n","            eval_freq=eval_freq,\n","            deterministic=deterministic,\n","            verbose=verbose,\n","            )\n","    self.trial = trial\n","    self.eval_idx = 0\n","    self.is_pruned = False\n","\n","  def _on_step(self) -> bool:\n","    if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n","      # Evaluate policy (done in the parent class)\n","      super()._on_step()\n","      self.eval_idx += 1\n","      # Send report to Optuna\n","      self.trial.report(self.last_mean_reward, self.eval_idx)\n","      # Prune trial if need\n","      if self.trial.should_prune():\n","        self.is_pruned = True\n","        return False\n","    return True"],"metadata":{"id":"BFISrW2ShZ7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def objective(trial: optuna.Trial) -> float:\n","  \"\"\"\n","  Objective function using by Optuna to evaluate\n","  one configuration (i.e., one set of hyperparameters).\n","\n","  Given a trial object, it will sample hyperparameters,\n","  evaluate it and report the result (mean episodic reward after training)\n","\n","  :param trial: Optuna trial object\n","  :return: Mean episodic reward after training\n","  \"\"\"\n","\n","  kwargs = DEFAULT_HYPERPARAMS.copy()\n","  ### YOUR CODE HERE\n","  # TODO:\n","  # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`\n","  # 2. Create the evaluation envs\n","  # 3. Create the `TrialEvalCallback`\n","\n","  # 1. Sample hyperparameters and update the keyword arguments\n","  kwargs.update(sample_a2c_params(trial))\n","\n","  # Create the RL model\n","  model = A2C(**kwargs)\n","\n","  # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n","  eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n","\n","  # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n","  # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n","  # TrialEvalCallback signature:\n","  # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n","  eval_callback = TrialEvalCallback(\n","                    eval_envs,\n","                    trial,\n","                    n_eval_episodes=N_EVAL_EPISODES,\n","                    eval_freq=EVAL_FREQ,\n","                    deterministic=True,\n","                    verbose=1,\n","                    )\n","\n","  ### END OF YOUR CODE\n","\n","  nan_encountered = False\n","  try:\n","    # Train the model\n","    model.learn(N_TIMESTEPS, callback=eval_callback)\n","  except AssertionError as e:\n","    # Sometimes, random hyperparams can generate NaN\n","    print(e)\n","    nan_encountered = True\n","  finally:\n","    # Free memory\n","    model.env.close()\n","    eval_envs.close()\n","\n","  # Tell the optimizer that the trial failed\n","  if nan_encountered:\n","    return float(\"nan\")\n","\n","  if eval_callback.is_pruned:\n","    raise optuna.exceptions.TrialPruned()\n","\n","  return eval_callback.last_mean_reward"],"metadata":{"id":"h84kCgVkiNre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch as th\n","\n","# Set pytorch num threads to 1 for faster training\n","th.set_num_threads(1)\n","# Select the sampler, can be random, TPESampler, CMAES, ...\n","sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n","# Do not prune before 1/3 of the max budget is used\n","pruner = MedianPruner(\n","            n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n","            )\n","# Create the study and start the hyperparameter optimization\n","study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n","\n","try:\n","  study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n","except KeyboardInterrupt:\n","  pass\n","\n","print(\"Number of finished trials: \", len(study.trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(f\"  Value: {trial.value}\")\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(f\"    {key}: {value}\")\n","\n","print(\"  User attrs:\")\n","for key, value in trial.user_attrs.items():\n","  print(f\"    {key}: {value}\")\n","\n","# Write report\n","study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n","\n","fig1 = plot_optimization_history(study)\n","fig2 = plot_param_importances(study)\n","\n","fig1.show()\n","fig2.show()"],"metadata":{"id":"Eg-FQKmQj_o_"},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8334,"status":"ok","timestamp":1703211771625,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"S5-k9Ypo9Ae3","outputId":"686ec958-7b1e-4882-a26e-02918ad08e18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n","Suggested packages:\n","  swig-doc swig-examples swig4.0-examples swig4.0-doc\n","The following NEW packages will be installed:\n","  swig swig4.0\n","0 upgraded, 2 newly installed, 0 to remove and 24 not upgraded.\n","Need to get 1,116 kB of archives.\n","After this operation, 5,542 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n","Fetched 1,116 kB in 0s (2,520 kB/s)\n","Selecting previously unselected package swig4.0.\n","(Reading database ... 121658 files and directories currently installed.)\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["!apt install swig cmake"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58728,"status":"ok","timestamp":1703211849041,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"QLDbkzXcJmzy","outputId":"629b9202-d4e4-47da-d54e-a96a6200b15d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting stable-baselines3==2.0.0a5 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading stable_baselines3-2.0.0a5-py3-none-any.whl (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting swig (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 2))\n","  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_sb3 (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4))\n","  Downloading huggingface_sb3-3.0-py3-none-any.whl (9.7 kB)\n","Collecting gymnasium==0.28.1 (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.23.5)\n","Requirement already satisfied: torch\u003e=1.11 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.0+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.2.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.5.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.7.1)\n","Collecting jax-jumpy\u003e=1.0.0 (from gymnasium==0.28.1-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n","Requirement already satisfied: typing-extensions\u003e=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.5.0)\n","Collecting farama-notifications\u003e=0.0.1 (from gymnasium==0.28.1-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","INFO: pip is looking at multiple versions of gymnasium[box2d] to determine which version is compatible with other requirements. This could take a while.\n","Collecting gymnasium[box2d] (from -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 3))\n","  Downloading gymnasium-0.29.0-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting box2d-py==2.3.5 (from gymnasium==0.28.1-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting pygame==2.1.3 (from gymnasium==0.28.1-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1))\n","  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub~=0.8 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (0.19.4)\n","Requirement already satisfied: pyyaml~=6.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (6.0.1)\n","Requirement already satisfied: wasabi in /usr/local/lib/python3.10/dist-packages (from huggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (1.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.13.1)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.31.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (4.66.1)\n","Requirement already satisfied: packaging\u003e=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.2.0)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (4.46.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.4.5)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (9.4.0)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (3.1.1)\n","Requirement already satisfied: python-dateutil\u003e=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.8.2)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2023.3.post1)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil\u003e=2.7-\u003ematplotlib-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.16.0)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (2.1.3)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (3.6)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003ehuggingface-hub~=0.8-\u003ehuggingface_sb3-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 4)) (2023.11.17)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.11-\u003estable-baselines3==2.0.0a5-\u003e-r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt (line 1)) (1.3.0)\n","Building wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349119 sha256=1713d6b2f6d131695986f05d5dd09d91cabf2ec74681de4348fd43b97e20e854\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: swig, farama-notifications, box2d-py, pygame, jax-jumpy, gymnasium, stable-baselines3, huggingface_sb3\n","  Attempting uninstall: pygame\n","    Found existing installation: pygame 2.5.2\n","    Uninstalling pygame-2.5.2:\n","      Successfully uninstalled pygame-2.5.2\n","Successfully installed box2d-py-2.3.5 farama-notifications-0.0.4 gymnasium-0.28.1 huggingface_sb3-3.0 jax-jumpy-1.0.0 pygame-2.1.3 stable-baselines3-2.0.0a5 swig-4.1.1.post1\n"]}],"source":["!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23708,"status":"ok","timestamp":1703211884315,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"lSwek2BjJ9Gr","outputId":"661eb553-3b30-4dbd-c904-a40b1ccc5c60"},"outputs":[{"name":"stdout","output_type":"stream","text":["\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n","Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n","Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n","Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n","Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n","Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n","Fetched 5,509 kB in 2s (3,369 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  freeglut3 libglu1-mesa\n","Suggested packages:\n","  libgle3 python3-numpy\n","The following NEW packages will be installed:\n","  freeglut3 libglu1-mesa python3-opengl\n","0 upgraded, 3 newly installed, 0 to remove and 27 not upgraded.\n","Need to get 824 kB of archives.\n","After this operation, 8,092 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n","Fetched 824 kB in 0s (2,882 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, \u003c\u003e line 3.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package freeglut3:amd64.\n","(Reading database ... 122411 files and directories currently installed.)\n","Preparing to unpack .../freeglut3_2.8.1-6_amd64.deb ...\n","Unpacking freeglut3:amd64 (2.8.1-6) ...\n","Selecting previously unselected package libglu1-mesa:amd64.\n","Preparing to unpack .../libglu1-mesa_9.0.2-1_amd64.deb ...\n","Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n","Selecting previously unselected package python3-opengl.\n","Preparing to unpack .../python3-opengl_3.1.5+dfsg-1_all.deb ...\n","Unpacking python3-opengl (3.1.5+dfsg-1) ...\n","Setting up freeglut3:amd64 (2.8.1-6) ...\n","Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n","Setting up python3-opengl (3.1.5+dfsg-1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common\n","The following NEW packages will be installed:\n","  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n","  xserver-common xvfb\n","0 upgraded, 9 newly installed, 0 to remove and 27 not upgraded.\n","Need to get 7,813 kB of archives.\n","After this operation, 11.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.5 [28.2 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.5 [863 kB]\n","Fetched 7,813 kB in 0s (15.8 MB/s)\n","Selecting previously unselected package libfontenc1:amd64.\n","(Reading database ... 125495 files and directories currently installed.)\n","Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n","Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Selecting previously unselected package libxfont2:amd64.\n","Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n","Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n","Selecting previously unselected package libxkbfile1:amd64.\n","Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n","Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Selecting previously unselected package x11-xkb-utils.\n","Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n","Unpacking x11-xkb-utils (7.7+5build4) ...\n","Selecting previously unselected package xfonts-encodings.\n","Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n","Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Selecting previously unselected package xfonts-utils.\n","Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n","Unpacking xfonts-utils (1:7.7+6build2) ...\n","Selecting previously unselected package xfonts-base.\n","Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n","Unpacking xfonts-base (1:1.0.5) ...\n","Selecting previously unselected package xserver-common.\n","Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.5_all.deb ...\n","Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.5) ...\n","Selecting previously unselected package xvfb.\n","Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.5_amd64.deb ...\n","Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.5) ...\n","Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n","Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n","Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n","Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n","Setting up x11-xkb-utils (7.7+5build4) ...\n","Setting up xfonts-utils (1:7.7+6build2) ...\n","Setting up xfonts-base (1:1.0.5) ...\n","Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.5) ...\n","Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.5) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}],"source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg\n","!apt install xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72UmJkZQKZJS"},"outputs":[],"source":["import os\n","\n","os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":631,"status":"ok","timestamp":1703211906479,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"s9tQ5quSKnyh","outputId":"e6b1fda9-1742-491b-ae2f-3e2bbfdf0274"},"outputs":[{"data":{"text/plain":["\u003cpyvirtualdisplay.display.Display at 0x7884771cd420\u003e"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9999,"status":"ok","timestamp":1703211918604,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"UxXnu5GFK8Fb"},"outputs":[],"source":["import gymnasium\n","\n","from huggingface_sb3 import load_from_hub, package_to_hub\n","from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy\n","from stable_baselines3.common.monitor import Monitor"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":267,"status":"ok","timestamp":1703211921912,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"JyXsnc1zMOIT","outputId":"9b0393d0-e868-4f23-85b0-b0da21eee45a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action taken: 2\n","Action taken: 0\n","Action taken: 2\n","Action taken: 0\n","Action taken: 0\n","Action taken: 3\n","Action taken: 1\n","Action taken: 0\n","Action taken: 1\n","Action taken: 1\n","Action taken: 1\n","Action taken: 0\n","Action taken: 2\n","Action taken: 1\n","Action taken: 2\n","Action taken: 1\n","Action taken: 3\n","Action taken: 0\n","Action taken: 1\n","Action taken: 3\n"]}],"source":["import gymnasium as gym\n","\n","# First, we create our environment called LunarLander-v2\n","env = gym.make(\"LunarLander-v2\")\n","\n","# Then we reset this environment\n","observation, info = env.reset()\n","\n","for _ in range(20):\n","  # Take a random action\n","  action = env.action_space.sample()\n","  print(\"Action taken:\", action)\n","\n","  # Do this action in the environment and get\n","  # next_state, reward, terminated, truncated and info\n","  observation, reward, terminated, truncated, info = env.step(action)\n","\n","  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n","  if terminated or truncated:\n","    # Reset the environment\n","    print(\"Environment is reset\")\n","    observation, info = env.reset()\n","\n","env.close()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1703211927138,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"8g5IcRj9UCPk","outputId":"0cc2a7e8-232d-4408-ce9a-3652d313fe1e"},"outputs":[{"name":"stdout","output_type":"stream","text":["_____OBSERVATION SPACE_____ \n","\n","Observation Space Shape (8,)\n","Sample observation [  3.5080276  -34.27243     -1.351712    -1.7087725    2.7070966\n","  -1.8507972    0.09038945   0.33233845]\n"]}],"source":["# We create our environment with gym.make(\"\u003cname_of_the_environment\u003e\")\n","env = gym.make(\"LunarLander-v2\")\n","env.reset()\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space Shape\", env.observation_space.shape)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280,"status":"ok","timestamp":1703211929045,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"PUA6x8Tt4f7B","outputId":"9f67182a-f902-41b6-f798-c1f5f01ebb9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," _____ACTION SPACE_____ \n","\n","Action Space Shape 4\n","Action Space Sample 1\n"]}],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":256,"status":"ok","timestamp":1703224835502,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"jz0mNuTO_D6n"},"outputs":[],"source":["# Create the environment\n","ENV_ID = 'LunarLander-v2'\n","N_EVAL_ENVS = 16\n","\n","env = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":264,"status":"ok","timestamp":1703224837698,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"8B45bM5LURgx"},"outputs":[],"source":["N_TRIALS = 200       # Maximum number of trials\n","N_STARTUP_TRIALS = 5    # Stop random sampling after N_STARTUP_TRIALS\n","N_EVALUATIONS = 1     # Number of evaluations during the training\n","N_TIMESTEPS = int(1e5)   # Training budget\n","EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n","N_EVAL_EPISODES = 10\n","TIMEOUT = int(60 * 60 * 5)  # 5 hours\n","\n","DEFAULT_HYPERPARAMS = {\n","            \"policy\": \"MlpPolicy\",\n","            \"env\": ENV_ID,\n","            }"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7006,"status":"ok","timestamp":1703211952564,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"xSyQG0gHIVJI","outputId":"a6f2961d-1423-4a02-ad2b-4d46762f55f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic\u003e=1.5.0 (from optuna)\n","  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n","Requirement already satisfied: sqlalchemy\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic\u003e=1.5.0-\u003eoptuna)\n","  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=4 in /usr/local/lib/python3.10/dist-packages (from alembic\u003e=1.5.0-\u003eoptuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy\u003e=1.3.0-\u003eoptuna) (3.0.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako-\u003ealembic\u003e=1.5.0-\u003eoptuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 optuna-3.5.0\n"]}],"source":["# Fine-tune hyperparameters\n","!pip install optuna"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1710,"status":"ok","timestamp":1703212000962,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"2b8_SyJ1JKbg"},"outputs":[],"source":["import optuna\n","from optuna.pruners import MedianPruner\n","from optuna.samplers import TPESampler\n","from optuna.visualization import plot_optimization_history, plot_param_importances"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":296,"status":"ok","timestamp":1703224844379,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"MR54TzDDJMi_"},"outputs":[],"source":["from typing import Any, Dict\n","\n","def sample_ppo_params(trial: optuna.Trial) -\u003e Dict[str, Any]:\n","  \"\"\"\n","  Sampler for PPO hyperparameters.\n","\n","  \"\"\"\n","\n","  learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1, log=True)\n","  n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 11) # 8, 16, 32, ... 1024, 2048\n","  batch_size = 2 ** trial.suggest_int(\"exponent_batch_size\", 3, 9) # 8, 16, 32, ... 512\n","  n_epochs = trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20])\n","  gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n","  gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n","  ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n","  vf_coef = trial.suggest_float(\"vf_coef\", 0, 1)\n","  clip_range = trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4])\n","  max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n","\n","  # Display true values\n","  trial.set_user_attr(\"gamma_\", gamma)\n","  trial.set_user_attr(\"n_steps\", n_steps)\n","\n","  net_arch_type = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\", \"medium\"])\n","\n","  net_arch = {\n","        \"tiny\": dict(pi=[64], vf=[64]),\n","        \"small\": dict(pi=[64, 64], vf=[64, 64]),\n","        \"medium\": dict(pi=[256, 256], vf=[256, 256]),\n","        }[net_arch_type]\n","\n","  activation_fn_name = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n","  activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn_name]\n","\n","  ortho_init = trial.suggest_categorical('ortho_init', [False, True])\n","\n","\n","  return {\n","      \"learning_rate\": learning_rate,\n","      \"n_steps\": n_steps,\n","      \"batch_size\": batch_size,\n","      \"gamma\": gamma,\n","      \"gae_lambda\": gae_lambda,\n","      \"ent_coef\": ent_coef,\n","      \"vf_coef\": vf_coef,\n","      \"clip_range\": clip_range,\n","      \"n_epochs\": n_epochs,\n","      \"max_grad_norm\": max_grad_norm,\n","      \"policy_kwargs\": dict(\n","                  net_arch=net_arch,\n","                  activation_fn=activation_fn,\n","                  ortho_init=ortho_init,\n","                 ),\n","      }"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":269,"status":"ok","timestamp":1703224851243,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"_8kDGFFIR-gY"},"outputs":[],"source":["from stable_baselines3.common.callbacks import EvalCallback\n","\n","class TrialEvalCallback(EvalCallback):\n","  \"\"\"\n","  Callback used for evaluating and reporting a trial.\n","  \"\"\"\n","\n","  def __init__(\n","        self,\n","        eval_env: gym.Env,\n","        trial: optuna.Trial,\n","        n_eval_episodes: int = 5,\n","        eval_freq: int = 10000,\n","        deterministic: bool = True,\n","        verbose: int = 0,\n","        ):\n","\n","    super().__init__(\n","            eval_env=eval_env,\n","            n_eval_episodes=n_eval_episodes,\n","            eval_freq=eval_freq,\n","            deterministic=deterministic,\n","            verbose=verbose,\n","            )\n","    self.trial = trial\n","    self.eval_idx = 0\n","    self.is_pruned = False\n","\n","  def _on_step(self) -\u003e bool:\n","    if self.eval_freq \u003e 0 and self.n_calls % self.eval_freq == 0:\n","      # Evaluate policy (done in the parent class)\n","      super()._on_step()\n","      self.eval_idx += 1\n","      # Send report to Optuna\n","      self.trial.report(self.last_mean_reward, self.eval_idx)\n","      # Prune trial if need\n","      if self.trial.should_prune():\n","        self.is_pruned = True\n","        return False\n","    return True"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1703224855804,"user":{"displayName":"Weiming Li","userId":"10689989110268100587"},"user_tz":-480},"id":"lNQc2NCcSfzY"},"outputs":[],"source":["def objective(trial: optuna.Trial) -\u003e float:\n","  \"\"\"\n","  Objective function using by Optuna to evaluate\n","  one configuration (i.e., one set of hyperparameters).\n","\n","  Given a trial object, it will sample hyperparameters,\n","  evaluate it and report the result (mean episodic reward after training)\n","\n","  :param trial: Optuna trial object\n","  :return: Mean episodic reward after training\n","  \"\"\"\n","\n","  kwargs = DEFAULT_HYPERPARAMS.copy()\n","\n","  # 1. Sample hyperparameters and update the keyword arguments\n","  kwargs.update(sample_ppo_params(trial))\n","\n","  # Create the RL model\n","  model = PPO(**kwargs)\n","\n","  # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n","  eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n","\n","  # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n","  # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n","  # TrialEvalCallback signature:\n","  # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n","  eval_callback = TrialEvalCallback(\n","                    eval_envs,\n","                    trial,\n","                    n_eval_episodes=N_EVAL_EPISODES,\n","                    eval_freq=EVAL_FREQ,\n","                    deterministic=True,\n","                    verbose=1,\n","                    )\n","\n","  nan_encountered = False\n","  try:\n","    # Train the model\n","    model.learn(N_TIMESTEPS, callback=eval_callback)\n","  except AssertionError as e:\n","    # Sometimes, random hyperparams can generate NaN\n","    print(e)\n","    nan_encountered = True\n","  finally:\n","    # Free memory\n","    model.env.close()\n","    eval_envs.close()\n","\n","  # Tell the optimizer that the trial failed\n","  if nan_encountered:\n","    return float(\"nan\")\n","\n","  if eval_callback.is_pruned:\n","    raise optuna.exceptions.TrialPruned()\n","\n","  return eval_callback.last_mean_reward"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"gJNldWKQUoBA"},"outputs":[{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:01:00,805] A new study created in memory with name: no-name-8d05df90-53b5-4e12-9878-f4be33ac2fda\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-567.12 +/- 113.38\n","Episode length: 61.40 +/- 7.03\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:03:51,803] Trial 0 finished with value: -567.1156289 and parameters: {'learning_rate': 0.16423997609209984, 'exponent_n_steps': 10, 'exponent_batch_size': 9, 'n_epochs': 10, 'gamma': 0.012276899790833823, 'gae_lambda': 0.98, 'ent_coef': 1.6689249623366034e-08, 'vf_coef': 0.325646694414656, 'clip_range': 0.4, 'max_grad_norm': 1, 'net_arch': 'medium', 'activation_fn': 'tanh', 'ortho_init': True}. Best is trial 0 with value: -567.1156289.\n","/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 512, but because the `RolloutBuffer` is of size `n_steps * n_envs = 256`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 256\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=256 and n_envs=1)\n","\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-744.75 +/- 290.20\n","Episode length: 122.70 +/- 37.59\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:06:33,685] Trial 1 finished with value: -744.7536779 and parameters: {'learning_rate': 4.9901380658024126e-05, 'exponent_n_steps': 8, 'exponent_batch_size': 9, 'n_epochs': 1, 'gamma': 0.00618533651298298, 'gae_lambda': 0.8, 'ent_coef': 5.047654543688845e-08, 'vf_coef': 0.9790263910162251, 'clip_range': 0.1, 'max_grad_norm': 0.7, 'net_arch': 'small', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 0 with value: -567.1156289.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-889.03 +/- 116.13\n","Episode length: 201.00 +/- 39.99\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:09:25,695] Trial 2 finished with value: -889.0328067 and parameters: {'learning_rate': 0.00043232375060323427, 'exponent_n_steps': 9, 'exponent_batch_size': 9, 'n_epochs': 10, 'gamma': 0.0022092989068896686, 'gae_lambda': 0.95, 'ent_coef': 1.5738925972143127e-06, 'vf_coef': 0.5309371761160323, 'clip_range': 0.1, 'max_grad_norm': 0.9, 'net_arch': 'tiny', 'activation_fn': 'relu', 'ortho_init': False}. Best is trial 0 with value: -567.1156289.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-97.47 +/- 33.62\n","Episode length: 1000.00 +/- 0.00\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:18:38,808] Trial 3 finished with value: -97.4738281 and parameters: {'learning_rate': 8.718880841054518e-05, 'exponent_n_steps': 10, 'exponent_batch_size': 4, 'n_epochs': 10, 'gamma': 0.0009392272934836159, 'gae_lambda': 0.92, 'ent_coef': 3.0724581826307985e-06, 'vf_coef': 0.8551260751291307, 'clip_range': 0.4, 'max_grad_norm': 2, 'net_arch': 'small', 'activation_fn': 'relu', 'ortho_init': True}. Best is trial 3 with value: -97.4738281.\n","/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 512, but because the `RolloutBuffer` is of size `n_steps * n_envs = 64`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 64\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=64 and n_envs=1)\n","\n","[I 2023-12-22 06:21:28,220] Trial 4 finished with value: -580.6654704999999 and parameters: {'learning_rate': 0.9474442388513654, 'exponent_n_steps': 6, 'exponent_batch_size': 9, 'n_epochs': 1, 'gamma': 0.012297988823800458, 'gae_lambda': 0.99, 'ent_coef': 0.00025966223167505124, 'vf_coef': 0.9085286744532104, 'clip_range': 0.2, 'max_grad_norm': 0.9, 'net_arch': 'small', 'activation_fn': 'relu', 'ortho_init': False}. Best is trial 3 with value: -97.4738281.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-580.67 +/- 185.42\n","Episode length: 70.60 +/- 23.20\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:29:06,359] Trial 5 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-734.45 +/- 350.03\n","Episode length: 112.30 +/- 37.87\n","New best mean reward!\n","Eval num_timesteps=100000, episode_reward=-477.27 +/- 377.13\n","Episode length: 111.10 +/- 31.31\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 06:42:15,140] Trial 6 finished with value: -477.2721478 and parameters: {'learning_rate': 0.005487082720149944, 'exponent_n_steps': 11, 'exponent_batch_size': 4, 'n_epochs': 20, 'gamma': 0.000488140550450165, 'gae_lambda': 0.92, 'ent_coef': 1.9801656759913627e-05, 'vf_coef': 0.06616626869424358, 'clip_range': 0.4, 'max_grad_norm': 0.6, 'net_arch': 'medium', 'activation_fn': 'relu', 'ortho_init': True}. Best is trial 3 with value: -97.4738281.\n","[I 2023-12-22 06:48:58,673] Trial 7 finished with value: -140.87730570000002 and parameters: {'learning_rate': 0.0020519760443140117, 'exponent_n_steps': 6, 'exponent_batch_size': 5, 'n_epochs': 10, 'gamma': 0.09261125627855146, 'gae_lambda': 1.0, 'ent_coef': 0.00043759419932262115, 'vf_coef': 0.7552424839196865, 'clip_range': 0.4, 'max_grad_norm': 0.3, 'net_arch': 'tiny', 'activation_fn': 'relu', 'ortho_init': True}. Best is trial 3 with value: -97.4738281.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-140.88 +/- 27.82\n","Episode length: 979.70 +/- 60.90\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=8 and n_envs=1)\n","\n","[I 2023-12-22 06:57:05,556] Trial 8 finished with value: -20.463924299999995 and parameters: {'learning_rate': 0.00018722998676140083, 'exponent_n_steps': 3, 'exponent_batch_size': 6, 'n_epochs': 5, 'gamma': 0.0010657394012677283, 'gae_lambda': 0.9, 'ent_coef': 1.3553962174079695e-06, 'vf_coef': 0.32803536668929567, 'clip_range': 0.3, 'max_grad_norm': 0.8, 'net_arch': 'small', 'activation_fn': 'tanh', 'ortho_init': True}. Best is trial 8 with value: -20.463924299999995.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-20.46 +/- 165.32\n","Episode length: 320.30 +/- 126.51\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=8 and n_envs=1)\n","\n","[I 2023-12-22 07:04:42,666] Trial 9 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-634.37 +/- 162.74\n","Episode length: 70.30 +/- 10.50\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","\n","[I 2023-12-22 07:08:56,435] Trial 10 finished with value: 39.9177404 and parameters: {'learning_rate': 0.0003557246619115638, 'exponent_n_steps': 5, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.06941341755614307, 'gae_lambda': 0.9, 'ent_coef': 0.023479957389460293, 'vf_coef': 0.013295029456668273, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 10 with value: 39.9177404.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=39.92 +/- 129.69\n","Episode length: 430.00 +/- 168.39\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 128, but because the `RolloutBuffer` is of size `n_steps * n_envs = 16`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 16\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=16 and n_envs=1)\n","\n","[I 2023-12-22 07:14:08,613] Trial 11 finished with value: 33.0013157 and parameters: {'learning_rate': 0.00045874308718450686, 'exponent_n_steps': 4, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.08000470232267169, 'gae_lambda': 0.9, 'ent_coef': 0.09945377433171733, 'vf_coef': 0.015036732395389078, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 10 with value: 39.9177404.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=33.00 +/- 74.31\n","Episode length: 268.10 +/- 78.81\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:18:27,964] Trial 12 finished with value: -23.078154000000005 and parameters: {'learning_rate': 0.001845845160064013, 'exponent_n_steps': 5, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.0967478990983189, 'gae_lambda': 0.9, 'ent_coef': 0.0956122507900787, 'vf_coef': 0.012622659854496222, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 10 with value: 39.9177404.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-23.08 +/- 128.09\n","Episode length: 379.40 +/- 201.39\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:22:43,308] Trial 13 finished with value: 91.42360350000001 and parameters: {'learning_rate': 0.0006518968311621037, 'exponent_n_steps': 5, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.03507408440564755, 'gae_lambda': 0.9, 'ent_coef': 0.005928367068416807, 'vf_coef': 0.13124266582235897, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=91.42 +/- 140.03\n","Episode length: 361.00 +/- 205.80\n","New best mean reward!\n","Eval num_timesteps=100000, episode_reward=-15.29 +/- 132.94\n","Episode length: 591.70 +/- 296.72\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:28:31,709] Trial 14 finished with value: -15.286058300000002 and parameters: {'learning_rate': 0.009708334734768452, 'exponent_n_steps': 7, 'exponent_batch_size': 6, 'n_epochs': 20, 'gamma': 0.027161999258839154, 'gae_lambda': 0.9, 'ent_coef': 0.004284323639505513, 'vf_coef': 0.1708874205319717, 'clip_range': 0.2, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n","/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","\n","[I 2023-12-22 07:32:20,778] Trial 15 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-951.92 +/- 406.70\n","Episode length: 178.30 +/- 99.28\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 128`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 128\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=128 and n_envs=1)\n","\n","[I 2023-12-22 07:36:05,733] Trial 16 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-163.54 +/- 45.38\n","Episode length: 1000.00 +/- 0.00\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:39:55,397] Trial 17 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-630.16 +/- 146.87\n","Episode length: 101.00 +/- 30.57\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 64`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 64\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=64 and n_envs=1)\n","\n","[I 2023-12-22 07:46:13,458] Trial 18 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-173.34 +/- 67.86\n","Episode length: 569.50 +/- 352.58\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:49:19,973] Trial 19 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-846.23 +/- 280.10\n","Episode length: 132.80 +/- 41.90\n","New best mean reward!\n","Eval num_timesteps=100000, episode_reward=57.63 +/- 134.91\n","Episode length: 512.20 +/- 89.15\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 07:53:46,176] Trial 20 finished with value: 57.625043500000004 and parameters: {'learning_rate': 0.0009288656691885226, 'exponent_n_steps': 7, 'exponent_batch_size': 5, 'n_epochs': 5, 'gamma': 0.01910542305690119, 'gae_lambda': 0.9, 'ent_coef': 0.0009060716696585576, 'vf_coef': 0.08899303734562777, 'clip_range': 0.3, 'max_grad_norm': 0.6, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n","[I 2023-12-22 07:58:45,215] Trial 21 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-93.00 +/- 17.12\n","Episode length: 1000.00 +/- 0.00\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:02:43,149] Trial 22 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-475.13 +/- 54.26\n","Episode length: 884.90 +/- 51.38\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:08:38,956] Trial 23 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-139.27 +/- 34.91\n","Episode length: 501.70 +/- 318.99\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:15:21,122] Trial 24 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-122.65 +/- 20.88\n","Episode length: 862.10 +/- 275.90\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:18:28,006] Trial 25 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-134.82 +/- 26.70\n","Episode length: 69.90 +/- 13.29\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:23:43,086] Trial 26 finished with value: 10.612410899999999 and parameters: {'learning_rate': 0.0003337512884720801, 'exponent_n_steps': 4, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.0028638213002966082, 'gae_lambda': 0.9, 'ent_coef': 0.0024127464944752736, 'vf_coef': 0.39030650342767803, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=10.61 +/- 67.27\n","Episode length: 325.10 +/- 109.26\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:27:36,341] Trial 27 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-2230.67 +/- 847.46\n","Episode length: 500.00 +/- 95.64\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","\n","[I 2023-12-22 08:30:41,080] Trial 28 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-561.11 +/- 79.88\n","Episode length: 319.30 +/- 23.21\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:33:58,096] Trial 29 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-575.48 +/- 95.79\n","Episode length: 70.50 +/- 14.40\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:43:02,631] Trial 30 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-83.97 +/- 15.54\n","Episode length: 849.00 +/- 304.33\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 08:48:17,581] Trial 31 finished with value: 67.5276804 and parameters: {'learning_rate': 0.00020520268684304222, 'exponent_n_steps': 4, 'exponent_batch_size': 7, 'n_epochs': 5, 'gamma': 0.0701017674847537, 'gae_lambda': 0.9, 'ent_coef': 0.07217947080152161, 'vf_coef': 0.02303966826475036, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=67.53 +/- 154.76\n","Episode length: 386.40 +/- 104.53\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=8 and n_envs=1)\n","\n","[I 2023-12-22 08:55:45,658] Trial 32 finished with value: 38.9360629 and parameters: {'learning_rate': 0.00016201595697365763, 'exponent_n_steps': 3, 'exponent_batch_size': 8, 'n_epochs': 5, 'gamma': 0.0628386590846626, 'gae_lambda': 0.9, 'ent_coef': 0.028992973139246418, 'vf_coef': 0.07765257246156065, 'clip_range': 0.3, 'max_grad_norm': 0.5, 'net_arch': 'tiny', 'activation_fn': 'tanh', 'ortho_init': False}. Best is trial 13 with value: 91.42360350000001.\n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=38.94 +/- 97.53\n","Episode length: 327.60 +/- 127.42\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 16`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 16\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=16 and n_envs=1)\n","\n","[I 2023-12-22 09:01:20,441] Trial 33 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-185.80 +/- 147.64\n","Episode length: 925.00 +/- 152.88\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 09:05:37,482] Trial 34 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-648.15 +/- 178.59\n","Episode length: 792.30 +/- 113.33\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 512, but because the `RolloutBuffer` is of size `n_steps * n_envs = 16`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 16\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=16 and n_envs=1)\n","\n","[I 2023-12-22 09:08:49,453] Trial 35 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-1197.01 +/- 389.16\n","Episode length: 223.00 +/- 61.44\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["[I 2023-12-22 09:14:49,680] Trial 36 pruned. \n"]},{"name":"stdout","output_type":"stream","text":["Eval num_timesteps=100000, episode_reward=-155.25 +/- 273.01\n","Episode length: 412.00 +/- 129.82\n","New best mean reward!\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning:\n","\n","You have specified a mini-batch size of 16, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=8 and n_envs=1)\n","\n"]}],"source":["import torch as th\n","from torch import nn as nn\n","\n","# Set pytorch num threads to 1 for faster training\n","th.set_num_threads(1)\n","# Select the sampler, can be random, TPESampler, CMAES, ...\n","sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n","# Do not prune before 1/3 of the max budget is used\n","pruner = MedianPruner(\n","            n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n","            )\n","# Create the study and start the hyperparameter optimization\n","study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n","\n","try:\n","  study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)\n","except KeyboardInterrupt:\n","  pass\n","\n","print(\"Number of finished trials: \", len(study.trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(f\"  Value: {trial.value}\")\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(f\"    {key}: {value}\")\n","\n","print(\"  User attrs:\")\n","for key, value in trial.user_attrs.items():\n","  print(f\"    {key}: {value}\")\n","\n","# Write report\n","study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n","\n","fig1 = plot_optimization_history(study)\n","fig2 = plot_param_importances(study)\n","\n","fig1.show()\n","fig2.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6ymOcYTDyDM"},"outputs":[],"source":["# SOLUTION\n","# We added some parameters to accelerate the training\n","model = PPO(\n","      policy = 'MlpPolicy',\n","      env = env,\n","      n_steps = 1024,\n","      batch_size = 64,\n","      n_epochs = 4,\n","      gamma = 0.999,\n","      gae_lambda = 0.98,\n","      ent_coef = 0.01,\n","      verbose=1\n","      )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLrYjJWUEOeK"},"outputs":[],"source":["# SOLUTION\n","# Train it for 1,000,000 timesteps\n","model.learn(total_timesteps=int(1e6))\n","# Save the model\n","model_name = \"ppo-LunarLander-v2\"\n","model.save(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lfqfKKvgHf_E"},"outputs":[],"source":["#@title\n","eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n","print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98tQigeMIP-L"},"outputs":[],"source":["notebook_login()\n","!git config --global credential.helper store\n","\n","# !huggingface-cli login --token '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'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNDavUGhtxMm"},"outputs":[],"source":["import gymnasium as gym\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import DummyVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","\n","from huggingface_sb3 import package_to_hub\n","\n","# PLACE the variables you've just defined two cells above\n","# Define the name of the environment\n","env_id = \"LunarLander-v2\"\n","\n","# TODO: Define the model architecture we used\n","model_architecture = \"PPO\"\n","\n","## Define a repo_id\n","## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n","## CHANGE WITH YOUR REPO ID\n","repo_id = \"Weiming1122/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine 😄\n","\n","## Define the commit message\n","commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n","\n","# Create the evaluation env and set the render_mode=\"rgb_array\"\n","eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n","\n","# PLACE the package_to_hub function you've just filled here\n","package_to_hub(model=model,              # Our trained model\n","        model_name=model_name,         # The name of our trained model\n","        model_architecture=model_architecture, # The model architecture we used: in our case PPO\n","        env_id=env_id,             # Name of the environment\n","        eval_env=eval_env,           # Evaluation Environment\n","        repo_id=repo_id,            # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n","        commit_message=commit_message)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9UWNGDD2Py3"},"outputs":[],"source":["!pip install shimmy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uedkeo3U3tUY"},"outputs":[],"source":["from huggingface_sb3 import load_from_hub\n","repo_id = \"Classroom-workshop/assignment2-omar\" # The repo_id\n","filename = \"ppo-LunarLander-v2.zip\" # The model filename.zip\n","\n","# When the model was trained on Python 3.8 the pickle protocol is 5\n","# But Python 3.6, 3.7 use protocol 4\n","# In order to get compatibility we need to:\n","# 1. Install pickle5 (we done it at the beginning of the colab)\n","# 2. Create a custom empty object we pass as parameter to PPO.load()\n","custom_objects = {\n","          \"learning_rate\": 0.0,\n","          \"lr_schedule\": lambda _: 0.0,\n","          \"clip_range\": lambda _: 0.0,\n","          }\n","\n","checkpoint = load_from_hub(repo_id, filename)\n","model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IbyJoE94XFH"},"outputs":[],"source":["#@title\n","eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n","mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n","print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNkGx8Y7baf2z1hIk01dqpz","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}